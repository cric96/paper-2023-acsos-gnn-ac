% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2018/03/10
%
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{acronym}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}
\def\doi#1{\href{https://doi.org/\detokenize{#1}}{\url{https://doi.org/\detokenize{#1}}}}
%
\usepackage{graphicx}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}
%
\usepackage{listings}

\usepackage{cleveref}
\lstset{language=Pascal}
\begin{document}
%
\title{Field-informed Reinforcement Learning for Learning Large Scale Collective Tasks} %% Todo improve
% or: Field-Informed Reinforcement Learning: A Scalable and Effective Method for Collective Intelligence
% or: A Novel Approach to Reinforcement Learning for Adaptive Collective Systems
%

\author{
\IEEEauthorblockN{Gianluca Aguzzi}
\IEEEauthorblockA{
%\textit{Department of Computer Science and Engineering} \\
\textit{%Alma Mater Studiorum--
University of Bologna}\\
Cesena, Italy\\
gianluca.aguzzi@unibo.it
}

\and
%\linebreakand %\and
\IEEEauthorblockN{Mirko Viroli}
\IEEEauthorblockA{
%\textit{Department of Computer Science and Engineering} \\
\textit{%Alma Mater Studiorum--
University of Bologna}\\
Cesena, Italy\\
mirko.viroli@unibo.it % 0000−0003−2702−5702
}
\and
\IEEEauthorblockN{Lukas Esterle}
\IEEEauthorblockA{
%\textit{Department of Computer Science and Engineering} \\
\textit{%Alma Mater Studiorum--
Aarhus University}\\
Aarhus, Denmark\\
lukas.esterle@ece.au.dk}
}
%
\maketitle              % typeset the header of the contribution
%

%%% Comment command, to be removed before submission
\newcommand{\meta}[3]{\textcolor{#1}{\textbf{#2}: #3}}
\newcommand{\ga}[1]{\meta{red}{GA}{#1}}
\newcommand{\lukas}[1]{\meta{purple}{Lukas}{#1}}
\newcommand{\mv}[1]{\meta{green}{MV}{#1}}
%% ACRONYMS
\acrodef{DecPOMDP}[DecPOMDP]{decentralized partially observable Markov decision processes}
\acrodef{RL}[RL]{reinforcement learning}
\acrodef{MARL}[MARL]{multi-agent reinforcement learning}
\acrodef{MAARL}[ManyRL]{many-agent reinforcement learning}
\acrodef{MDP}[MDP]{Markov Decision Process}
\acrodef{MLP}[MLP]{multi-layer perceptron}
\acrodef{GNN}[GNN]{graph neural network}
\acrodef{CNN}[CNN]{convolutional neural network}
\acrodef{CTDE}[CTDE]{centralized training and decentralized execution}
\acrodef{DQN}[DQN]{Deep Q-Learning}
\ga{Page limit: 10 pages (include refs)}

\begin{abstract}
Coordinatinating a group of intelligent agents in multi-agent systems 
 is a research problem that has been addressed for a long time, 
 due to the challenges posed by distribution and the definition of distributed intelligence. 
%
The problem is even more evident in collective adaptive systems, 
 where the scale of the systems considered makes the definition of collective behaviours even more challenging. 

In this paper, we consider a new reinforcement learning approach 
  for the definition of intelligent agents called \emph{Field-Informed reinforcement learning}, 
  where we use computational \emph{computational fields} to manage the interaction between agents 
  and Graph Neural Networks to learn a local behaviour necessary to solve collective tasks. 
%
This allows us to create distributed controllers informed by a collective knowledge 
 distilled during learning but that use only local information at runtime.
% 
We demonstrate the effectiveness of this new approach in a simulated use case 
 where coordination tasks are successfully solved. \lukas{do we really have multiple case studies?} 
\end{abstract}

\begin{IEEEkeywords}
aggregate computing, Graph Neural Network, Cyber-Physical Swarms.
\end{IEEEkeywords}
%
\ga{Paper levels:
\begin{itemize}
  \item Distributed collective intelligence
  \item GNN and field coordination
  \item Spatial tracking
\end{itemize}
}
\section{Introduction}
Various phenomena in the real world are not evenly distributed across the environment such as watershed, wild fires, traffic, human crowd movement, or wild life movement from insects, fish, birds, and mammals.
To acquire appropriate information about such phenomena, we also need to distribute the sensors accordingly to cover all aspects sufficiently. While manual deployment will mitigate the problem, not all phenomena are known exactly \emph{a priori} and would require maintenance and adjustment during runtime. Even worse, with phenomena able to change their location, size, and distribution---as it is the case with crowds, wild life, or wild fires---an adaptation of the collective sensors is required in order to keep the required information. 

This leads to several questions 
\begin{enumerate}
	\item How to distribute sensors to ensure good attainment of information?
	\item How to maintain knowledge about the phenomena and unlearn irrelevant information?
	\item How to move sensors when the phenomena moves to ensure good coverage?
\end{enumerate} \lukas{refine and rework these questions}

In this paper, we propose a novel approach combining aggregate computing~\cite{Beal2015Computer} with Graph Neural Networks~\cite{Zhou2020AIOpen} to focus on the relevant areas. We further combine this with reinforcement learning techniques to respond to changing conditions in the environment. 
%
This follow a novel trends in which high-level declarative programming language are mixed with machine learning technique in order to synthesise robust collective controllers~\cite{DBLP:conf/acsos/Aguzzi21,DBLP:conf/icdcs/AguzziCV22,DBLP:conf/coordination/AguzziCV22}.
%Specifically, we will show... \lukas{more info...}
Specifically, 
 our approach utilizes aggregate computing to disseminate information by manipulating a computational field, 
 which is a distributed data structure that maps devices to information. 
 This field functions as a dynamic environment in which information is continuously updated and diffused throughout the system. 
 By leveraging this layer, we can build collective distributed intelligence using \ac{GNN},
  which uses the information in the field to determine the best action for a given task.
\ga{first draft, need a refinement}
The remainder of this paper is structured as follows. First, we introduce the relevant background and problem formulation in \Cref{sec:background}. Afterwards, we introduce our approach in \Cref{sec:approach}. \Cref{sec:eval} outlines the performed experiments and discusses the obtained results. Finally, we will present our conclusions in \Cref{sec:conclusion}.


\section{Background and Motivation}
\label{sec:background}
In this section, we aim to provide a concise overview of the system we intend to address, 
 starting with a system definition. 
%
We then proceed to discuss relevant research areas that inform our approach. 
%
Finally, we present a formalization of the problem and a discussion of the motivations behind our proposed work.
\ga{Page budget: 1/2 page \\}
\ga{Plan: discussion about the problem of coordination in multi-agent systems 
and the need for a scalable approach. In doing this, we will discuss some of the existing approaches (declarative, RL and )}
\subsection{System definition}
This article focuses on the definition of \emph{decentralized collective intelligence} within the context of \emph{large-scale} distributed systems, 
 specifically what we call \emph{Cyber-Physical Swarms} or \emph{swarm-like systems}
 that are systems of \emph{devices} (or nodes) equipped with \emph{sensors} and \emph{actuator}
 each of which can only interact with the \emph{1-hop} neighbourhood that can 
 be logical (i.e., a communication network) or physical (i.e., a spatial neighbourhood).
%
Each node executes a \emph{local} controller that can only access the information available to its \emph{context}
 that is the information available in the neighbourhood and the information available from its sensors.
%
After each program evaluation, 
 the node can send messages to its neighbourhood about its current evaluation and update its internal state.
%
This general model can be adapted to several different scenarios, 
 such as a network of sensors, a network of robots (e.g., swarm robotics), or a network of mobile phones.
%
Our goal is to find a \emph{homogeneously} distributed controller $\pi$ (i.e., the same controller for the whole system) that, 
 starting from \emph{only} local configurations, leads 
 the system to achieve a certain \emph{collective} requirements through \emph{cooperation}, 
 such as spatial area coverage, phenomena tracking, and robot aggregation.
%
The homogeneity of the controller is a key requirement to ensure the \emph{scalability} of the approach, 
 as it allows us to avoid the need for a centralised controller that would be a bottleneck for the system,
 and it is the typical choice in swarm-like systems~\cite{brambilla2013swarm,yang2021many,pmlr-v80-yang18d,DBLP:conf/aaai/ZhengYCZZWY18}.
%
\subsection{Field Coordination Approaches}
\ga{discussion about field used as a coordination mechanism in multi-agent systems. Introduction to AC}
The field-based coordination approaches utilize a concept of \emph{computational fields} 
 (or simply \emph{fields}), 
 which are \emph{distributed} data structures that associate locations with values and evolve over time. 
%
A modern field-coordination approach called \emph{aggregate computing}~\cite{Beal2015Computer}
 is rooted in earlier work on \emph{artificial potential fields}~\cite{DBLP:conf/icra/Warren89} and \emph{co-fields}~\cite{DBLP:journals/pervasive/MameiZL04}. 
% 
 In this \emph{macro-programming}~\cite{DBLP:journals/corr/abs-2201-03473} paradigm is possible to devise collective and self-organizing behaviour through a composition of functions operating on fields. 
%
The fields map a set of individual agents to computational values, 
 allowing them to associate what they sense, process, and actuate. 
%
Fields are computed locally but subject to a global viewpoint, 
 enabling emergent collective behaviour through the interplay of the system model (similar to the above described) 
 and the \emph{programming model} known as field calculus~\cite{DBLP:conf/esocc/ViroliDB13,DBLP:journals/corr/ViroliADPB16,DBLP:journals/tocl/AudritoVDPB19}.
\subsection{Graph Neural Networks}
Over the years, several different neural networks have been proposed to solve specific tasks, ranging from 
  simple \ac{MLP}~\cite{werbos1975beyond} to \ac{CNN}~\cite{oshea2015introduction}.
%
Whereas \ac{CNN} is designed to be used with image and spatial-like data (e.g., audio),
 \ac{GNN} is a novel model used to process graph-structured data. 
%
Let $G = (V, E)$ a graph  
where $E$ defines the neighbourhood relations for each participating node, 
and $V$ identifies the nodes present in the graph at that time. 
% 
Each node $v$ is associated with an observation (or feature set), $o_v$, 
%
Given $o_v$ a feature vector associated with each node $v \in V$, 
 the goal of a \ac{GNN} is to learn the node embedding $h_v$ for each node $v \in V$.
 In modern \acp{GNN}, the node embedding $h_v$ is computed by aggregating information from the node's neighbours $N(v)$,
  and then combining it with the node's current embedding $h_v$.
% 
Formally, a \ac{GNN} can be defined by two phases:
\begin{equation}
a_{v}^{(k)}=\operatorname{AGGREGATE}^{(k)}\left(\left\{h_{u}^{(k-1)}: u \in \mathcal{N}(v)\right\}\right)   
\end{equation}
\begin{equation}
h_{v}^{(k)}=\operatorname{COMBINE}^{(k)}\left(h_{v}^{(k-1)}, a_{v}^{(k)}\right)
\end{equation}
where $h_{v}^{k}$ is the embedding of node $v$ at the $k$-th layer ($h_v^0 = o_v$), 
 $\mathcal{N}(v)$ is the set of neighbors of node $v$ computed from $E$.
%
The differential part come into play in the $\operatorname{COMBINE}$ function, 
 which is usually a differentiable function such as a neural network layer.
%
$\operatorname{AGGREGATE} $ instead is a function that aggregates the information from the neighbours of a node $v$
  and it could be a simple sum or a more complex function such as a neural network layer.
%
This formulation allows GNNs to effectively process and extract features from graph-structured data by iteratively aggregating and transforming information from the node's neighbours.

\acp{GNN} are used in several fields such as social network analysis, chemistry, and physics.
In this paper, we use GNNs to learn a local behaviour for each agent in a multi-agent system (more details in Section~\ref{sec:approach}).
\subsection{Many-Agent Reinforcement Learning}
\ga{plan: we could discuss briefly about standard approach (few agents), and mean-field approach (many agents). 
This will lead to our approach, which is a combination of DQN (or any Deep RL approach) and GNN.}

\Ac{RL} has gained a lot of interest recently, 
 thanks to its successful application in various scenarios, 
 ranging from video games (such as Alpha Go~\cite{Silver2016Go} and Atari~\cite{Atari2016DQN}) 
 to chatbots (like ChatGPT~\cite{ChatGPT2023}). 
% 
In \ac{RL}, an agent (i.e., a smart entity capable of making decisions) 
 performs actions in an environment (i.e., everything outside the agent) according to a policy, 
 to maximize long-term rewards.

One interesting application of \ac{RL} is when there are multiple learning agents involved. 
 Such scenarios are referred to as multi-agent reinforcement learning (MARL)~\cite{zhang2019marl}. 
%
In particular, in this work, we consider homogenous \ac{MAARL}~\cite{yang2021many}, 
 where $N \gg 2$ and each agent is interchangeable and indistinguishable.
%
This research area is relevant in the context of large-scale systems 
 when the incentive of the individual is reduced compared to the collective behaviour of the system, 
 which is found in collective adaptive systems such as swarm robotics.

In fact, the first works in this direction were discussed precisely in the last field, 
 exploring new models (e.g., swarMDP~\cite{DBLP:conf/atal/SosicKZK17}) and learning algorithms capable of extrapolating 
 a policy equal to the entire system. 
Modern approaches, however, 
 have started to consider the use of deep learning approaches to try 
 to synthesize robust controllers capable of generalizing to new tasks. 
In this context, mean-field reinforcement learning~\cite{pmlr-v80-yang18d} is certainly noteworthy. 
 Mean-field RL is a technique for learning approximate inference in large-scale systems, 
 where the interactions between the agents can be decoupled into a single conditional probability distribution function for each agent, 
 thus simplifying the learning process and reducing the computational cost.
%
Some known approaches using mean-field reinforcement learning include Q-mean, 
 which is an extension of Q-learning to mean-field settings, 
 and actor-critic mean-field, which combines actor-critic algorithms with mean-field approximations. 
%
These approaches have shown promising results in various domains, such as multi-agent coordination 
 and decentralized control, and are actively being researched and developed for further applications
\subsection{Problem formalisation}
Given the homogeneity and large scale considered and the \emph{locality} (i,e., each agent can only observe its neighbours), 
 the problem can be modelled through the SwarMDP model~\cite{DBLP:conf/atal/SosicKZK17} -- 
 an extension of the \ac{DecPOMDP}~\cite{Decpomdp2000} model to swarm-like systems.
% 
A SwarMDP is characterized by a \emph{swarming agent} ($\mathbb{A}$) and the dynamics of the environment ($\mathbb{E}$).
Specifically, $\mathbb{A}$ is a tuple ($\mathcal{S}, \mathcal{O}, \mathcal{A}, \mathcal{R}, \pi$) where:
\begin{itemize}
  \item $\mathcal{S, O, A}$ are the set of local states, observations (or features), and actions, respectively;
  \item $\mathcal{R}: \mathcal{S} \rightarrow \mathbb{R}$ is the reward function, which is influenced by the environment;
  \item $\pi: \mathcal{O} \rightarrow \mathcal{A}$ is the policy function, which maps the observations to the actions. It could be deterministic or stochastic.
\end{itemize}
Starting from this definition, the environment $\mathbb{E}$ is defined as a tuple ($\mathcal{P}, \mathbb{A}, \mathcal{T}, \xi$), where:
\begin{itemize}
  \item $\mathcal{P}$ is the total number of agents in the systems (the agent population), which is assumed to be fixed;
  \item $\mathbb{A}$ is the above defined agent prototype that rules each agent $v \in P$;
  \item $\mathcal{T}: \mathcal{S}^P \times \mathcal{A}^P \times \mathcal{S}^P \rightarrow \mathbb{R}$ is the transition  global function, which is influenced by the actions of the agents and returns a collective reward -- this is typically not known by the swarming agents;
  \item $\xi: \mathcal{S^P} \rightarrow \mathcal{O^P}$ is the global observation model of the systems.
\end{itemize}
In swarMDP, the neighbourhood is not directly defined, but it is implicitly defined by the observation model $\xi$.
Since in our case, the agents can only interact with 1-hop neighbourhoods and they aren't directly influenced by other 
  agent observations, we restrict the observation model as follows:
\begin{equation}
\xi(v): \{s_j, j \in \mathcal{N}^v\} \rightarrow O
\end{equation}
\begin{equation}
\xi = \{\xi(v), v \in \mathcal{P}\}
\end{equation}
where $\mathcal{N}^v$ is the set of neighbours of $v$.

This model can be used then to express the evolution of the system in time.
Specifically, starting from a global state $\mathcal{S}^P_t$, the next state $\mathcal{S}^P_{t+1}$ is defined as:
\begin{equation}
\mathcal{A}^P_t = \pi(\xi(S^P_t))
\end{equation}
\begin{equation}
\mathcal{S}^P_{t+1} = \mathcal{T}(\mathcal{S}^P_t, \mathcal{A}^P_t)
\end{equation}
Given a time $t$, the system can be also represented as a graph $G_t = (V_t, E_t)$, 
  where $E_t$ is built from $\mathcal{N}$.
  Each node is then decorated with the local observation $o_v$. 
  Therefore, this graph can be used for being processed by a \ac{GNN} as done in~\cite{DBLP:conf/corl/TolstayaGPP0R19,tolstaya2020learning,DBLP:conf/icra/GosrichMLPYR022}.
\subsection{Motivation}
The definition of distributed controllers for intelligent systems can be divided into two major approaches: 
 \emph{manual design}, where the application is conceived and described through a programming language, 
 and \emph{automatic design}~\cite{brambilla2013swarm}, which employs machine learning techniques to synthesize distributed controllers. 
%
Our work lies between these two approaches, specifically in the areas of field-based coordination and \ac{MAARL} with the use of \acp{GNN}.

Compared to earlier works on field-based coordination, 
 our approach builds on the concept of co-fields~\cite{DBLP:journals/pervasive/MameiZL04}, 
 where agents exploit the field as a stigmergic medium to receive system-wide information and apply reasoning to this data.
%  
We present a subsequent approach where agent intelligence is synthesized through \ac{MAARL} and the use of \ac{GNN} to learn a local representation from the neighbourhood.
%
By learning a smart policy directly in the environment, the agent becomes capable of adapting its behaviour to new situations.

The use of \acp{GNN} as part of a distributed controller has been explored in previous literature~\cite{DBLP:conf/icra/GosrichMLPYR022,DBLP:conf/corl/TolstayaGPP0R19}, 
 where it was shown that they could be used to break down the evaluation of local and distributed systems. 
%
However, in these works, communication was left entirely to the neural network, 
 making the learning process potentially more complex and unstable. 
 In our approach, the \ac{GNN} is \emph{informed} by computational fields that collect the necessary information to compute a certain task, 
 limiting learning to only the specific task defined by a collective reward function.
 This will speed up the learning process and make it more stable.
\section{Field-informed Reinforcement Learning}
\label{sec:approach}
\ga{Page budget: 2/3 pages \\}
\ga{Plan: we should discuss the approach in detail, including the system model, and the learning dynamics.
Particularly, I would like to highlight how each node in the graph is a local controller,
but it could be seen as a global evolution, therefore we can use global information to inform the local controller.}
\begin{figure*}
  \includegraphics[width=\textwidth]{images/architecture.pdf}
  \label{fig:architecture}
  \caption{High-level description of Field-Informed reinforcement learning dynamics}
\end{figure*}
In this section, we aim to clarify the components involved in our proposed solution (i.e., architecture), how these components interact with each other to bring the system to perform the collective behavior (i.e dynamics), and finally, we will detail the learning algorithm designed and used to synthesize the policy (learning algorithm).
\subsection{Architecture and System dynamics}
\ga{Plan: 
  discussion about the typical AC evaluation model, specifying how GNN (with 1-hop information diffusion)
  can be easily integrated with this distributed model
}
The proposed solution consists mainly of two parts, 
\emph{i)} the aggregate program used to create part of the observation and 
\emph{ii)} the policy $\pi_{gnn}$ learned through \ac{GNN}-based learning. 
%
Let $\Gamma$ be the aggregate program that runs against $G_t$. 
%
The evaluation of $\Gamma$  produce a field value $\theta^v_t$ for each node $v$ in $G_t$. 
The feature vector $o_v$ of which is composed the graph $G'_t$ used by the policy $\pi_{gnn}$ is described by:
 $o_v = (\theta^v_t, \omega)$.
 where $\omega$ are other local information gather from sensors (like temperature, distances from neighbourhood, etc.). 
%
The policy $\pi_{gnn}$ is then evaluated for each node, 
 producing an action $a_t$ that will then modify the global state of the system.
 \ga{todo: put pseudo code to express this dynamics}
%
Despite appearing centralized, this system proposes a completely decentralized execution. 
%
In fact, the program $\Gamma$ is proactively executed at every node, 
and the \ac{GNN} can be locally evaluated using only neighborhood information. 
%
We want to emphasize that, in this case, the \acp{GNN} must be 1-hop; 
otherwise, they could not have a local interpretation for each node, according to our system model.
\subsection{Learning algorithm}
\ga{
  Plan: discuss the chosen approach, that is Centralised Training and Distributed Execution.
  Particularly, we use DQN in which the neural network is a GNN. So, the dataset consists in a set of graphs,
  where each graph is a snapshot of the system. 
  The target is the Q-value of the action taken in the current state.
  Explain how this can be then used for distributed controllers.
}
In swarm-like systems, 
 the typical approach for learning follows a strategy known as \ac{CTDE}. \ga{perhaps it should be introduced in many RL} 
The idea is to learn a policy at simulation time where there is a collective view of the system, 
 and then at runtime use the policy found with global information. 
This approach allows policies that are influenced by global information but only require local information to function at runtime. 
%
The typical approach in such cases is based on actor-critic systems~\cite{DBLP:conf/nips/LoweWTHAM17,wu2022more,song2022ctds,song2022centralized},
  where the \emph{actor} is the distributed policy (with only local information) and the critic is a neural network that takes the overall system state.

In our case, we leveraged the property of \ac{GNN}s to have a dual interpretation, i.e., to function globally over the entire graph and locally only over the neighborhood. 
%
Therefore, we relied on the value-based approach \ac{DQN}~\cite{mnih2015playing} \ga{to introduce in many rl??} with two major modifications: 
\begin{enumerate}
  \item experience replay stores experiences in the form of \emph{graphs},
  \item the neural network used to compute the Q function is based on a \ac{GNN} with an \ac{MLP} downstream.
\end{enumerate}
The first point is a natural extension due to the fact that we work on graphs rather than simple values. 
%
For the second point,
 the use of \acp{GNN} allows us to define policies on a variable neighborhood  that is essential in such systems as this can vary based on the neighborhood policy,
 and it is known that \acp{GNN} have a certain ability to generalize to new structures and scale with different nodes. 
% 
Additionally, 
 using the overall graph compared to local experiences makes learning more stable as it reduces the non-stationarity of the environment perceived by each node.
% 
 During learning, it is as if we have a \emph{single} agent collectively choosing which action to take (i.e., resulting action graph). 
%
The algorithm is described in the following pseudocode.

\section{Evaluation}
\label{sec:eval}
\ga{Page budget: 2 pages \\}
\ga{Plan: we should discuss the robot aggregation scenario, specifying the state space, the action space, and the reward function.
I dunno if it makes sense to discuss the variants (i.e., fixed area, two areas and moving area). We will see.
}
\subsection{Scenario}

\paragraph*{Variant A}
\paragraph*{Variant B}
\paragraph*{Variant C}

\subsection{Discussion and Results}
\section{Conclusion and Future Work}
\ga{Page budget: 0.5}
\label{sec:conclusion}

\bibliographystyle{IEEEtran}
\bibliography{bibliography}

\end{document}
