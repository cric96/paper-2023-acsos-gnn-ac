% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2018/03/10
%
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage{cleveref}
\usepackage{acronym}
\def\doi#1{\href{https://doi.org/\detokenize{#1}}{\url{https://doi.org/\detokenize{#1}}}}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}
%
\usepackage{listings}
\lstset{language=Pascal}
\begin{document}
%
\title{Field-informed Reinforcement Learning for Learnining Large Scale Collective Tasks} %% Todo improve
% or: Field-Informed Reinforcement Learning: A Scalable and Effective Method for Collective Intelligence
% or: A Novel Approach to Reinforcement Learning for Adaptive Collective Systems
%

\author{
\IEEEauthorblockN{Gianluca Aguzzi}
\IEEEauthorblockA{
%\textit{Department of Computer Science and Engineering} \\
\textit{%Alma Mater Studiorum--
University of Bologna}\\
Cesena, Italy\\
gianluca.aguzzi@unibo.it
}

\and
%\linebreakand %\and
\IEEEauthorblockN{Mirko Viroli}
\IEEEauthorblockA{
%\textit{Department of Computer Science and Engineering} \\
\textit{%Alma Mater Studiorum--
University of Bologna}\\
Cesena, Italy\\
mirko.viroli@unibo.it % 0000−0003−2702−5702
}
\and
\IEEEauthorblockN{Lukas Esterle}
\IEEEauthorblockA{
%\textit{Department of Computer Science and Engineering} \\
\textit{%Alma Mater Studiorum--
Aarhus University}\\
Aarhus, Denmark\\
lukas.esterle@ece.au.dk}
}
%
\maketitle              % typeset the header of the contribution
%

%%% Comment command, to be removed before submission
\newcommand{\meta}[3]{\textcolor{#1}{\textbf{#2}: #3}}
\newcommand{\ga}[1]{\meta{red}{GA}{#1}}
\newcommand{\lukas}[1]{\meta{purple}{Lukas}{#1}}
\newcommand{\mv}[1]{\meta{green}{MV}{#1}}
\ga{Page limit: 10 pages (include refs)}

\acrodef{DecPOMDP}[DecPOMDP]{decentralized partially observable Markov decision processes}

\begin{abstract}
Coordinatinating a group of intelligent agents in multi-agent systems 
 is a research problem that has been addressed for a long time, 
 due to the challenges posed by distribution and of the definition of distributed intelligence. 
%
The problem is even more evident in collective adaptive systems, 
 where the scale of the systems considered makes the definition of collective behaviors even more challenging. 

In this paper we consider a new reinforcement learning approach 
  for the definition of intelligent agents called \emph{Field-Informed reinforcement learning}, 
  where we use computational \emph{computational fields} to manage the interaction between agents 
  in stigmergically and Graph Neural Networks to learn a local behaviour necessary to solve collective tasks. 
%
This allows us to create distributed controllers informed by a collective knowledge 
 distilled during learning but that use only local information at runtime.
% 
We demonstrate the effectiveness of this new approach in several case studies 
 where coordination tasks are successfully solved. \lukas{do we really have multiple case studies?} 
\end{abstract}

\begin{IEEEkeywords}
aggregate computing, Graph Neural Network, Cyber-Physical Swarms.
\end{IEEEkeywords}
%
\section{Introduction}
Various phenomena in the real world are not evenly distributed across the environment such as watershed, wild fires, traffic, human crowd movement, or wild life movement from insects, fish, birds, and mammals.
To acquire appropriate information about such phenomena, we also need to distribute the sensors accordingly to cover all aspects sufficiently. While manual deployment will mitigate the problem, not all phenomena are known exactly \emph{a priori} and would require maintenance and adjustment during runtime. Even worse, with phenomena able to change their location, size, and distribution---as it is the case with crowds, wild life, or wild fires---an adaptation of the collective sensors is required in order to keep the required information. 

This leads to several questions 
\begin{enumerate}
	\item How to distribute sensors to ensure good attainment of information?
	\item How to maintain knowledge about the phenomena and unlearn irrelevant information?
	\item How to move sensors when the phenomena moves to ensure good coverage?
\end{enumerate} \lukas{refine and rework these questions}

In this paper, we propose a novel approach combining aggregate computing
~\cite{Beal2015Computer} with Graph Neural Networks
~\cite{Zhou2020AIOpen} to focus on the relevant areas. We further combine this with reinforcement learning techniques to respond to changing conditions in the environment.
Specifically, we will show... \lukas{more info...}

The remainder of this paper is structured as follows. First we introduce the relevant background and problem formulation in \Cref{sec:background}. Afterwards we introduce our approach in Section~\Cref{sec:approach}. Section~\Cref{sec:eval} outlines the performed experiments and discusses the obtained results. Finally, we will present our conclusions in Section~\Cref{sec:conclusion}.


\section{Background and Motivation}
\label{sec:background}
\ga{Page budget: 1/2 page \\}
\ga{Plan: discussion about the problem of coordination in multi-agent systems 
and the need for a scalable approach. In doing this, we will discuss some of the existing approaches (declarative, RL and )}
\subsection{Field Coordination Approaches}
\ga{discussion about field used as a coordination mechanism in multi-agent systems. Introduction to AC}
\subsection{Graph Neural Networks for Multi-Agent Systems}
\subsection{Many-Agent Reinforcement Learning}
\ga{plan: we could discuss briefly about standard appraoch (few agents), mean field approach (many agents). This will led to our approach, which is a combination of DQN (or any Deep RL approach) and GNN.}

Reinforcement learning (RL) has gained a lot of interest recently, 
 thanks to its successful application in various scenarios, 
 ranging from video games (such as Alpha Go and Atari) 
 to chatbots (like ChatGPT). 
% 
In RL, an agent (i.e., a smart entity capable of making decisions) 
 performs actions in an environment (i.e., everything outside the agent) according to a policy, 
 with the goal of maximizing long-term rewards.

One interesting application of RL is when there are multiple learning agents involved. 
 Such scenarios are referred to as multi-agent reinforcement learning (MARL). 
 While standard RL approaches use Markov decision processes to model the world, 
 in MARL for distributed systems, the notion of 
 \acp{DecPOMDP} is used.

A \ac{DecPOMDP} represents a sequential game, 
 in which each agent receives a local observation about the world they 
 inhabit and receives a joint immediate reward in response to the actions they take. 
% 
\ga{Improve, if necessary, the following paragraph.}
Formally, a DecPOMDP is defined as a tuple $\langle N, S, \{A_i\}, P, R, \{\Omega_i\}, O, \gamma \rangle$, 
 where $N$ is the total number of agents that live in the environment, 
 $S$ is the state set of the environment, 
 $\{A_i\}$ is the set of actions for agent $i$ and $A$ is the joint action space, 
 $P$ is the probability transition function, 
 $R$ is the immediate joint reward function, and 
 $\{\Omega_i\}$ is the set observation function for the agent $i$,
 $O$ is the observation space, and finally $\gamma$ is the discount factor.
%
On top of this model, each agent try to learn a policy $\pi_i$ in order 
 to maximise the long term join reward signal.

In base of some of this concept, 
 we have several different MARL, like cooperative MARL (i.e., the reward function incetivize collective outcomes)
 heterogenous MARL (i.e., the agents have different action space and policy) and
 homogenous MARL (i.e., the agents have the same action space and policy).
%  
In particular, in this work we consider homogenous many agent RL, 
 where, N >> 2 and action space of each agent and the policy is the same (i.e., the agents are interchangeable)
\section{Field-informed Reinforcement Learning}
\label{sec:approach}
\ga{Page budget: 2/3 pages \\}
\ga{Plan: we should discuss the approach in detail, including the system model, the learning dynamics.
Particularly, I would like to highlight how the each node in the graph is a local controller,
but it could be see as a global evolution, therefore we can use global information to inform the local controller.}
\subsection{System model}
\ga{Plan: 
  discussion about typical AC evalaution model, specifying how GNN (with 1-hop information diffusion)
  can be easily integrate with this distributed model
}
\subsection{Learning dynamics}
\ga{
  Plan: discuss about the choosing approach, that is Centralised Training and Distributed Execution.
  Particularly, we use DQN in which the neural network is a GNN. So, the dataset consist in a set of graphs,
  where each graph is a snapshot of the system. 
  The target is the Q-value of the action taken in the current state.
  Explain how this can be then use for distributed controller.
}
\section{Evaluation}
\label{sec:eval}
\ga{Page budget: 2 pages \\}
\ga{Plan: we should discuss the robot aggregation scenario, specifiying the state space, the action space, the reward function.
I dunno if it makes sense to discuss the variants (i.e., fixed area, two ares and moving area). We will see.
}
\subsection{Scenario}

\paragraph*{Variant A}
\paragraph*{Variant B}
\paragraph*{Variant C}

\subsection{Discussion and Results}
\section{Conclusion and Future Work}
\ga{Page budget: 0.5}
\label{sec:conclusion}

\bibliographystyle{IEEEtran}
\bibliography{bibliography}

\end{document}
